{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Explore destinations of travellers entering the U.S. with Spark and Redshift\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The objective of this project is to construct a model that gather information about traveller entering the U.S. and natural and social environment of  their destination state, which may be helpful to understand the immigration and population flow tendency. For example, many male travellers from country A immigrate to state B for work, where there is a good weather, a younger median age in population, or more foreign born residents, we may correlate the demographics situation to the immigration and find out the insight in further analysis.\n",
    "\n",
    "The project include the gather, access and clean data form various data sources and build a data lake using Spark, create ETL pipelines for final model  with Redshift\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, col,when, isnan, year, month, dayofmonth, hour, weekofyear, date_format, to_date, min, max,desc\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType,FloatType, StringType, IntegerType, DateType\n",
    "\n",
    "from datetime import date, datetime,timedelta\n",
    "\n",
    "import configparser\n",
    "import psycopg2\n",
    "import os\n",
    "from sql_queries import create_table_queries, drop_table_queries, copy_table_queries, insert_table_queries_dim, insert_table_queries_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "fname_immg = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "fname_immg_label = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "fname_tmpt = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "\n",
    "fname_dmgr = 'us-cities-demographics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\\nAWS_SECRET_ACCESS_KEY=config['AWS']['AWS_SECRET_ACCESS_KEY']\\n\\nos.environ['AWS_ACCESS_KEY_ID']= config['AWS']['AWS_SECRET_ACCESS_KEY']\\nos.environ['AWS_SECRET_ACCESS_KEY']= config['AWS']['AWS_SECRET_ACCESS_KEY']\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONFIG\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "AWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\"\"\"\n",
    "AWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark = SparkSession.builder.    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").    enableHiveSupport().getOrCreate()\\nspark._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", AWS_ACCESS_KEY)\\nspark._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", AWS_SECRET_KEY)\\n    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.7\"). \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11, org.apache.hadoop:hadoop-aws:2.7.5\").\\\n",
    "    config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", AWS_ACCESS_KEY_ID).\\\n",
    "    config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", AWS_SECRET_ACCESS_KEY).\\\n",
    "    enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test upload to s3\n",
    "\n",
    "df_dmgr = spark.read.options(delimiter=';',header='true').csv(fname_dmgr)\n",
    "#df_dmgr.write.parquet(\"s3a://awsbucketmxy666/capstone/\",mode=\"overwrite\")\n",
    "df_dmgr.write.parquet(\"s3a://aws-emr-resources-508919957385-us-east-2/cap/US_demographics\",mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The data source contain 1 SAS data and 2 CSV format source. The project create a data lake with these raw sources with big data frame Spark and store them into S3 bucket in AWS. Then, it preform a ETL process, extracting data from S3, transforming and loading the desired model into Redshift data warehouse, with which other analytic roles could perform further analysis  \n",
    "\n",
    "#### Describe and Gather Data \n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office and it includes information about details of travellers who land in the U.S.. This project picks the data of April 2016 as sample.\n",
    "More information: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "- World Temperature Data: This dataset came from Kaggle and it includes information about monthly average temperature of  main cities in the world. This project select data ranging from 1993 to 2013.\n",
    "More information: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "- U.S. City Demographic Data: This data comes from OpenSoft and includes demographical  information in the US at city level.\n",
    "More information: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- dataset 1: Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94 = spark.read.format('com.github.saurfang.sas.spark').load(fname_immg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here --TOO SLOW\n",
    "df = pd.read_sas(fname_immg, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    37.0      2.0    1.0      None     None  None       T    None   \n",
       "1      NaN    25.0      3.0    1.0  20130811      SEO  None       G    None   \n",
       "2  20691.0    55.0      2.0    1.0  20160401     None  None       T       O   \n",
       "3  20567.0    28.0      2.0    1.0  20160401     None  None       O       O   \n",
       "4  20567.0     4.0      2.0    1.0  20160401     None  None       O       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0       U    None   1979.0  10282016   None   None    None  1.897628e+09   \n",
       "1       Y    None   1991.0       D/S      M   None    None  3.736796e+09   \n",
       "2    None       M   1961.0  09302016      M   None      OS  6.666432e+08   \n",
       "3    None       M   1988.0  09302016   None   None      AA  9.246846e+10   \n",
       "4    None       M   2012.0  09302016   None   None      AA  9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0   None       B2  \n",
       "1  00296       F1  \n",
       "2     93       B2  \n",
       "3  00199       B2  \n",
       "4  00199       B2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(df_i94.count())\n",
    "df_i94.printSchema()\n",
    "df_i94.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- dataset 2: Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_tmpt = spark.read.options(header='True').csv(fname_tmpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8599212\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview the dataset\n",
    "print(df_tmpt.count())\n",
    "print(df_tmpt.show(5))\n",
    "df_tmpt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " - dateset 3: Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_dmgr = spark.read.options(delimiter=';',header='true').csv(fname_dmgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2891\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview the dataset\n",
    "print(df_dmgr.count())\n",
    "print(df_dmgr.show(5))\n",
    "df_dmgr.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc, and explore the distribution of values in the dataset to have an overview of their content.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- dataset 1.1: lable table: \n",
    "    - turn the \".sas\" file into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_i94_label = spark.read.format(\\'com.github.saurfang.sas.spark\\').load(fname_immg_label)\\ndf_i94_label = pd.read_sas(fname_immg_label, \\'sas7bdat\\', encoding=\"ISO-8859-1\") \\ndf_i94_label.head()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "\n",
    "# can't read the file using spark or pandas\n",
    "'''\n",
    "df_i94_label = spark.read.format('com.github.saurfang.sas.spark').load(fname_immg_label)\n",
    "df_i94_label = pd.read_sas(fname_immg_label, 'sas7bdat', encoding=\"ISO-8859-1\") \n",
    "df_i94_label.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# map a section of SAS file into a dictionary\n",
    "def code_mapper(file, idx):\n",
    "    f_content_clean = f_content[f_content.index(idx):]\n",
    "    f_content_clean = f_content_clean[:f_content_clean.index(';')].split('\\n')\n",
    "    f_content_clean = [i.replace(\"'\", \"\") for i in f_content_clean]\n",
    "    \n",
    "    dic = [i.split('=') for i in f_content_clean[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "# create dictionaries to map values into immigration\n",
    "i94cit_res = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = code_mapper(f_content, \"I94VISA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean the dictionary to reduce invalid data\n",
    "def clean_dic(dic,keywords):\n",
    "    for key, value in dic.items():\n",
    "        for keyword in keywords:\n",
    "            if value.startswith(keyword):\n",
    "                dic[key] = \"others\"\n",
    "    return dic\n",
    "\n",
    "keywords=[\"INVALID\",\"No Country Code\",\"Collapsed\",\"No PORT Code\"]\n",
    "i94cit_res=clean_dic(i94cit_res,keywords)\n",
    "i94port=clean_dic(i94port,keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " - dataset 1.2: main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cic_id</th>\n",
       "      <th>i94_year</th>\n",
       "      <th>i94_month</th>\n",
       "      <th>i94_incoming_country</th>\n",
       "      <th>i94_landing_port</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>i94_travel_mode</th>\n",
       "      <th>i94_address</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>i94_age</th>\n",
       "      <th>i94_visa</th>\n",
       "      <th>match_flag</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>admitted_date</th>\n",
       "      <th>gender</th>\n",
       "      <th>INS_no</th>\n",
       "      <th>airline</th>\n",
       "      <th>admission_no</th>\n",
       "      <th>flight_no</th>\n",
       "      <th>visa_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>ECUADOR</td>\n",
       "      <td>NOT REPORTED/UNKNOWN</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>None</td>\n",
       "      <td>1979</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1897628485</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>ATLANTA, GA</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>Air</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>Student</td>\n",
       "      <td>None</td>\n",
       "      <td>1991</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>ALBANIA</td>\n",
       "      <td>WASHINGTON DC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>MICHIGAN</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>60</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>M</td>\n",
       "      <td>1961</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>666643185</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>ALBANIA</td>\n",
       "      <td>NEW YORK, NY</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>33</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>M</td>\n",
       "      <td>1988</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>ALBANIA</td>\n",
       "      <td>NEW YORK, NY</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>9</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>M</td>\n",
       "      <td>2012</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cic_id  i94_year  i94_month i94_incoming_country      i94_landing_port  \\\n",
       "0       6      2016          4              ECUADOR  NOT REPORTED/UNKNOWN   \n",
       "1       7      2016          4                 None           ATLANTA, GA   \n",
       "2      15      2016          4              ALBANIA         WASHINGTON DC   \n",
       "3      16      2016          4              ALBANIA          NEW YORK, NY   \n",
       "4      17      2016          4              ALBANIA          NEW YORK, NY   \n",
       "\n",
       "  arrival_date i94_travel_mode    i94_address departure_date  i94_age  \\\n",
       "0   2016-04-29            None           None           None       42   \n",
       "1   2016-04-07             Air        ALABAMA           None       30   \n",
       "2   2016-04-01             Air       MICHIGAN     2016-08-25       60   \n",
       "3   2016-04-01             Air  MASSACHUSETTS     2016-04-23       33   \n",
       "4   2016-04-01             Air  MASSACHUSETTS     2016-04-23        9   \n",
       "\n",
       "   i94_visa match_flag  birth_year admitted_date gender INS_no airline  \\\n",
       "0  Pleasure       None        1979    2016-10-28   None   None    None   \n",
       "1   Student       None        1991          None      M   None    None   \n",
       "2  Pleasure          M        1961    2016-09-30      M   None      OS   \n",
       "3  Pleasure          M        1988    2016-09-30   None   None      AA   \n",
       "4  Pleasure          M        2012    2016-09-30   None   None      AA   \n",
       "\n",
       "   admission_no flight_no visa_type  \n",
       "0    1897628485      None        B2  \n",
       "1    2147483647     00296        F1  \n",
       "2     666643185        93        B2  \n",
       "3    2147483647     00199        B2  \n",
       "4    2147483647     00199        B2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Performing cleaning tasks\n",
    "# map the value in dictionary to columns\n",
    "get_i94cit_res = udf(lambda x: i94cit_res.get(str(int(x))) if x != None else x )\n",
    "get_i94port = udf(lambda x: i94port.get(x))\n",
    "get_i94mode = udf(lambda x: i94mode.get(str(int(x))) if x != None else x )\n",
    "get_i94addr = udf(lambda x: i94addr.get(x))\n",
    "get_i94visa = udf(lambda x: i94visa.get(str(int(x))) if x != None else x )\n",
    "\n",
    "# calculate the current age of recorded immigrant\n",
    "sas2dt = udf(lambda x: (datetime(1960,1,1) + timedelta(days = x)).strftime('%Y-%m-%d') if x != None else x )\n",
    "\n",
    "df_i94_clean =df_i94.withColumn('i94cit', get_i94cit_res(df_i94.i94cit)) \\\n",
    "                .withColumn('i94port', get_i94port(df_i94.i94port)) \\\n",
    "                .withColumn('i94mode', get_i94mode(df_i94.i94mode)) \\\n",
    "                .withColumn('i94addr', get_i94addr(df_i94.i94addr)) \\\n",
    "                .withColumn('i94visa', get_i94visa(df_i94.i94visa)) \\\n",
    "                .drop(\"i94res\",\"dtadfile\", \"count\",\"visapost\", \"occup\", \"entdepa\", \"entdepd\", \"entdepu\") \\\n",
    "                .withColumn(\"cicid\",col(\"cicid\").cast(IntegerType())) \\\n",
    "                .withColumn(\"i94yr\",col(\"i94yr\").cast(IntegerType())) \\\n",
    "                .withColumn(\"i94mon\",col(\"i94mon\").cast(IntegerType()))\\\n",
    "                .withColumn(\"biryear\",col(\"biryear\").cast(IntegerType()))\\\n",
    "                .withColumn(\"admnum\",col(\"admnum\").cast(IntegerType())) \\\n",
    "                .withColumn(\"i94bir\",date.today().year-col(\"biryear\")) \\\n",
    "                .withColumn(\"dtaddto\",to_date(col(\"dtaddto\"),\"MMddyyyy\")) \\\n",
    "                .withColumn(\"arrdate\",to_date(sas2dt(df_i94.arrdate))) \\\n",
    "                .withColumn(\"depdate\",to_date(sas2dt(df_i94.depdate))) \\\n",
    "                .withColumnRenamed(\"cicid\",\"cic_id\") \\\n",
    "                .withColumnRenamed(\"i94yr\",\"i94_year\") \\\n",
    "                .withColumnRenamed(\"i94mon\",\"i94_month\") \\\n",
    "                .withColumnRenamed(\"i94cit\",\"i94_incoming_country\") \\\n",
    "                .withColumnRenamed(\"i94port\",\"i94_landing_port\") \\\n",
    "                .withColumnRenamed(\"arrdate\",\"arrival_date\") \\\n",
    "                .withColumnRenamed(\"i94visa\",\"i94_visa\") \\\n",
    "                .withColumnRenamed(\"i94mode\",\"i94_travel_mode\") \\\n",
    "                .withColumnRenamed(\"i94addr\",\"i94_address\") \\\n",
    "                .withColumnRenamed(\"depdate\",\"departure_date\") \\\n",
    "                .withColumnRenamed(\"i94bir\",\"i94_age\") \\\n",
    "                .withColumnRenamed(\"matflag\",\"match_flag\") \\\n",
    "                .withColumnRenamed(\"biryear\",\"birth_year\") \\\n",
    "                .withColumnRenamed(\"dtaddto\",\"admitted_date\") \\\n",
    "                .withColumnRenamed(\"insnum\",\"INS_no\") \\\n",
    "                .withColumnRenamed(\"admnum\",\"admission_no\") \\\n",
    "                .withColumnRenamed(\"fltno\",\"flight_no\") \\\n",
    "                .withColumnRenamed(\"visatype\",\"visa_type\")\n",
    "\n",
    "df_i94_clean.limit(5).toPandas() # .select('i94cit_res').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## data exploratory by column\n",
    "# df_i94_clean.groupBy('i94addr').count().orderBy(desc('count')).show(10)\n",
    "df_i94_clean.groupBy('i94_incoming_country').count().orderBy(desc('count')).show()\n",
    "df_i94_clean.groupBy('match_flag').count().show()\n",
    "df_i94_clean.groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_i94_clean.write.parquet(\"i94_2016_apr\") #.partitionBy('i94addr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1743-11-01', '2013-09-01', 1993)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the range of tha date: 1743-2013\n",
    "min_date, max_date = df_tmpt.select(min(\"dt\"), max(\"dt\")).first()\n",
    "# set 20 year before last recorded year as start date\n",
    "start_year = datetime.strptime(max_date, '%Y-%m-%d').year - 20\n",
    "\n",
    "min_date, max_date, start_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- average_temperature: float (nullable = true)\n",
      " |-- average_temperature_uncertainty: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "870920\n",
      "+----------+-------------------+-------------------------------+-----+-------+--------+---------+----+-----+\n",
      "|      date|average_temperature|average_temperature_uncertainty| city|country|latitude|longitude|year|month|\n",
      "+----------+-------------------+-------------------------------+-----+-------+--------+---------+----+-----+\n",
      "|2003-04-01|              9.286|                           0.36|Çorlu| Turkey|  40.99N|   27.69E|2003|    4|\n",
      "|1995-08-01|             21.224|                          0.273|Çorum| Turkey|  40.99N|   34.08E|1995|    8|\n",
      "|1998-02-01|              1.314|                          0.228|Çorum| Turkey|  40.99N|   34.08E|1998|    2|\n",
      "+----------+-------------------+-------------------------------+-----+-------+--------+---------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "df_tmpt_clean=df_tmpt.where(df_tmpt.AverageTemperature!=\"null\") \\\n",
    "                    .withColumn(\"AverageTemperature\",col(\"AverageTemperature\").cast(FloatType())) \\\n",
    "                    .withColumn(\"AverageTemperatureUncertainty\",col(\"AverageTemperatureUncertainty\").cast(FloatType())) \\\n",
    "                    .withColumn(\"year\", year(df_tmpt.dt)) \\\n",
    "                    .withColumn(\"month\", month(df_tmpt.dt)) \\\n",
    "                    .withColumn(\"dt\",col(\"dt\").cast(DateType())) \\\n",
    "                    .withColumnRenamed(\"dt\",\"date\") \\\n",
    "                    .withColumnRenamed(\"AverageTemperature\",\"average_temperature\") \\\n",
    "                    .withColumnRenamed(\"AverageTemperatureUncertainty\",\"average_temperature_uncertainty\") \\\n",
    "                    .withColumnRenamed(\"City\",\"city\") \\\n",
    "                    .withColumnRenamed(\"Country\",\"country\") \\\n",
    "                    .withColumnRenamed(\"Latitude\",\"latitude\") \\\n",
    "                    .withColumnRenamed(\"Longitude\",\"longitude\")\n",
    "df_tmpt_clean = df_tmpt_clean.where(df_tmpt_clean.year >= start_year).distinct()\n",
    "\n",
    "df_tmpt_clean.printSchema()\n",
    "print(df_tmpt_clean.count())\n",
    "print(df_tmpt_clean.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Country|count|\n",
      "+-------------+-----+\n",
      "|        India|96968|\n",
      "|        China|94240|\n",
      "|United States|63992|\n",
      "|       Brazil|54560|\n",
      "|        Japan|43400|\n",
      "|       Russia|38688|\n",
      "|    Indonesia|35960|\n",
      "|       Mexico|24153|\n",
      "|      Nigeria|22568|\n",
      "|      Germany|20088|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## explore the distribution of the data by time and by location\n",
    "\n",
    "# df_tmpt_clean.describe(['Country']).show()\n",
    "# df_tmpt_clean.select('Country').distinct().count()\n",
    "df_tmpt_clean.groupBy('Country').count().orderBy(desc('count')).show(10)\n",
    "df_tmpt_clean.groupBy('month').count().orderBy(desc('count')).show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_tmpt_clean.write.parquet(\"global_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " - dateset 3: Demogrphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: float (nullable = true)\n",
      "\n",
      "2891\n",
      "               city          state state_code                       Race  \\\n",
      "0     Silver Spring       Maryland         MD         Hispanic or Latino   \n",
      "1            Quincy  Massachusetts         MA                      White   \n",
      "2            Hoover        Alabama         AL                      Asian   \n",
      "3  Rancho Cucamonga     California         CA  Black or African-American   \n",
      "4            Newark     New Jersey         NJ                      White   \n",
      "\n",
      "   count  male_population  female_population  median_age  total_population  \\\n",
      "0  25924            40601              41862   33.799999             82463   \n",
      "1  58723            44129              49500   41.000000             93629   \n",
      "2   4759            38040              46799   38.500000             84839   \n",
      "3  24437            88127              87105   34.500000            175232   \n",
      "4  76402           138040             143873   34.599998            281913   \n",
      "\n",
      "   number_of_veterans  foreign_born  average_household_size  \n",
      "0                1562         30908                    2.60  \n",
      "1                4147         32935                    2.39  \n",
      "2                4819          8229                    2.58  \n",
      "3                5821         33878                    3.18  \n",
      "4                5829         86253                    2.73  \n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks\n",
    "\n",
    "df_dmgr_clean =df_dmgr.withColumn(\"male_population\",col(\"Male Population\").cast(IntegerType())) \\\n",
    "                      .withColumn(\"female_population\",col(\"Female Population\").cast(IntegerType())) \\\n",
    "                      .withColumn(\"median_age\",col(\"Median Age\").cast(FloatType())) \\\n",
    "                      .withColumn(\"total_population\",col(\"Total Population\").cast(IntegerType()))\\\n",
    "                      .withColumn(\"number_of_veterans\",col(\"Number of Veterans\").cast(IntegerType()))\\\n",
    "                      .withColumn(\"foreign_born\",col(\"Foreign-born\").cast(IntegerType()))\\\n",
    "                      .withColumn(\"average_household_size\",col(\"Average Household Size\").cast(FloatType()))\\\n",
    "                      .withColumn(\"Count\",col(\"Count\").cast(IntegerType())) \\\n",
    "                      .withColumnRenamed(\"State Code\",\"state_code\") \\\n",
    "                      .withColumnRenamed(\"City\",\"city\") \\\n",
    "                      .withColumnRenamed(\"State\",\"state\") \\\n",
    "                      .withColumnRenamed(\"Count\",\"count\") \\\n",
    "                      .drop(\"Median Age\",\"Male Population\",\"Male Population\",\"Total Population\",\n",
    "                            \"Number of Veterans\",\"Foreign-born\",\"Average Household Size\",\"Female Population\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_dmgr_clean.printSchema()\n",
    "print(df_dmgr_clean.count())\n",
    "print(df_dmgr_clean.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "+--------------+-----+\n",
      "|         State|count|\n",
      "+--------------+-----+\n",
      "|    California|  676|\n",
      "|         Texas|  273|\n",
      "|       Florida|  222|\n",
      "|      Illinois|   91|\n",
      "|    Washington|   85|\n",
      "|      Colorado|   80|\n",
      "|       Arizona|   80|\n",
      "|      Michigan|   79|\n",
      "|North Carolina|   70|\n",
      "|      Virginia|   70|\n",
      "+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+--------------------+-----+\n",
      "|     State|                Race|count|\n",
      "+----------+--------------------+-----+\n",
      "|California|  Hispanic or Latino|  137|\n",
      "|California|               White|  137|\n",
      "|California|               Asian|  136|\n",
      "|California|Black or African-...|  136|\n",
      "|California|American Indian a...|  130|\n",
      "|     Texas|               White|   57|\n",
      "|     Texas|  Hispanic or Latino|   57|\n",
      "|     Texas|               Asian|   56|\n",
      "|     Texas|Black or African-...|   54|\n",
      "|     Texas|American Indian a...|   49|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## explore the distribution of the data by state, state and race\n",
    "\n",
    "print(df_dmgr_clean.select('State').distinct().count())\n",
    "df_dmgr_clean.groupBy('State').count().orderBy(desc('count')).show(10)\n",
    "df_dmgr_clean.groupBy('State','Race').count().orderBy(desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet: US_demographics\n",
    "df_dmgr_clean.write.parquet(\"US_demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create temperature_df in U.S. by joining democraphics_df\n",
    "df_city_state = df_dmgr_clean.select('city','state').distinct().withColumnRenamed(\"city\",\"City_\")\n",
    "\n",
    "df_tmpt_clean_us=df_tmpt_clean.where(df_tmpt_clean.country == \"United States\")\\\n",
    "                            .join(df_city_state,df_tmpt_clean.city ==  df_city_state.City_,\"left\") \\\n",
    "                            .drop(\"country\",\"City_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet: US_temperature table\n",
    "df_tmpt_clean_us.write.parquet(\"US_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- upload parquets into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# currently not able to directly store the data into S3, \n",
    "# thus the parquet files are uploaded manually\n",
    "\n",
    "\"\"\"\n",
    "import pathlib\n",
    "import boto3\n",
    "import configparser\n",
    "\n",
    "# BASE_DIR = pathlib.Path(__file__).parent.resolve()\n",
    "\n",
    "AWS_REGION = \"us-east-2\"\n",
    "S3_BUCKET_NAME = \"s3a://aws-emr-resources-508919957385-us-east-2/cap/\"\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "\n",
    "def upload_files(file_name, bucket, object_name=None, args=None):\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    s3_client.upload_file(file_name, bucket, object_name, ExtraArgs=args)\n",
    "    print(f\"'{file_name}' has been uploaded to '{S3_BUCKET_NAME}'\")\n",
    "\n",
    "upload_files(parquet_dmgr, S3_BUCKET_NAME) # f\"{BASE_DIR}/files/demo.txt\"\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "df_dmgr_clean.write.parquet(\"s3a://aws-emr-resources-508919957385-us-east-2/cap/US_demographics\",mode=\"overwrite\")\n",
    "df_dmgr_clean.write.parquet(\"s3://aws-emr-resources-508919957385-us-east-2/cap/US_demographics\",mode=\"overwrite\")\n",
    "df_dmgr_clean.write.parquet(\"s3a://awsbucketmxy666/capstone/\",mode=\"overwrite\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Star schema with 3 dimension table and 1 fact table: \n",
    "The final model needs a complex join operation of 3 sources and 2 of them require aggregation of source data into \"state\" level. it would be more clear and vulnerable to errors if the process divided into 2 steps: 1. extract and transform data into 3 dimension tables, 2 of which require to be aggregated, 2. join the transformed tables into the final fact table.\n",
    "\n",
    "fact table:\n",
    "- Information of travellers and their destination table: get the information of travellers who enter the U.S. and their destination state's demographical and geographical information. it might be helpful to understand some characters of immigration and population.\n",
    "\n",
    "Dimension table:\n",
    "- traveller table with basic identity information of travellers\n",
    "- temperature table in U.S. by state\n",
    "- Demographics table, including population and their distribution by sex and by race,  in U.S by state\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "- Create 3 stage tables using 3 parquet files in S3.\n",
    "- Extract information from stage tables into 3 dimension tables: traveller information table, temperature table from 1993 to 2013 aggregated by state, demographics tables aggregated by state\n",
    "- Extract information from 3 dimension tables into final fact table by join operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- 4.10 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- 4.11 Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop tables\n",
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "# create desired tables\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staging tables and query tables has been successfully created\n"
     ]
    }
   ],
   "source": [
    "drop_tables(cur, conn)\n",
    "create_tables(cur, conn)\n",
    "print(\"staging tables and query tables has been successfully created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- 4.12 stage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# copy data from s3 and insert data into staging tables\n",
    "def load_staging_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sources has been successfully loaded into staging tables\n"
     ]
    }
   ],
   "source": [
    "load_staging_tables(cur, conn)\n",
    "print(\"data sources has been successfully loaded into staging tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- 4.11 insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# insert data into dimension table from staging tables\n",
    "def insert_tables_dim (cur, conn):\n",
    "    for query in insert_table_queries_dim :\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "# insert data into fact table from dimension tables\n",
    "def insert_tables_fact (cur, conn):\n",
    "    for query in insert_table_queries_fact :\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data in the staging tables has been successfully inserted into query tables\n"
     ]
    }
   ],
   "source": [
    "insert_tables_dim(cur, conn)\n",
    "insert_tables_fact(cur, conn)\n",
    "print(\"data in the staging tables has been successfully inserted into query tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Quality checks to ensure the pipeline ran as expected, which include:\n",
    "- Integrity constraints on the relational database (e.g., unique key, correctness if join operation, etc.)\n",
    "- Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "# check the uniqueness of the key column\n",
    "def check_unique_key(dic,cur, conn):\n",
    "    query_count=\"\"\"\n",
    "                SELECT COUNT (*) \n",
    "                FROM {};\n",
    "                \"\"\"\n",
    "    query_count_unique_key=\"\"\"\n",
    "                SELECT COUNT ( DISTINCT {} ) \n",
    "                FROM {};\n",
    "                \"\"\"\n",
    "    query_count_sample=\"\"\"\n",
    "                SELECT *\n",
    "                FROM {}\n",
    "                LIMIT 3\n",
    "                \"\"\"\n",
    "    \n",
    "    for table, key in dic.items():\n",
    "        # check the completeness of the data\n",
    "        cur.execute(query_count.format(table))\n",
    "        res_count=cur.fetchone()[0]\n",
    "        if res_count<1:\n",
    "            raise Exception(f\"Sorry, no record is loaded into TABLE {table}\")\n",
    "\n",
    "        \n",
    "        # check the uniqueness of the key column and \n",
    "        cur.execute(query_count_unique_key.format(key,table))\n",
    "        res_count_uqnie_key=cur.fetchone()[0]\n",
    "        if res_count!=res_count_uqnie_key:\n",
    "            raise Exception(f\"Sorry, the {key} column is not unique in the TABLE {table}, please check!\")\n",
    "        \n",
    "        print(f\"congratulation! no error is found in uniqueness and completeness in the TABLE {table}\")\n",
    "        \n",
    "        # print sample data\n",
    "        cur.execute(query_count_sample.format(table))\n",
    "        res_sample=cur.fetchmany(3)\n",
    "        \n",
    "        print(\"sample data:\")\n",
    "        for r_s in res_sample:\n",
    "            print(r_s)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# check if the join operation is correctly performed\n",
    "def check_join(dic,cur, conn):\n",
    "    query_count=\"\"\"\n",
    "                SELECT COUNT (*) \n",
    "                FROM {};\n",
    "                \"\"\"\n",
    "    query_count_null=\"\"\"\n",
    "                SELECT COUNT (*) \n",
    "                FROM {}\n",
    "                WHERE {} IS NULL\n",
    "                \"\"\"\n",
    "    \n",
    "    query_count_sample=\"\"\"\n",
    "                SELECT *\n",
    "                FROM {}\n",
    "                LIMIT 3\n",
    "                \"\"\"\n",
    "    \n",
    "    for table, columns in dic.items():\n",
    "        # count the number of records\n",
    "        cur.execute(query_count.format(table))\n",
    "        res_count=cur.fetchone()[0]\n",
    "        \n",
    "        for column in columns:\n",
    "            # count null values in assigned column\n",
    "            cur.execute(query_count_null.format(table, column))\n",
    "            res_count_null=cur.fetchone()[0]\n",
    "            \n",
    "            # if more than half of the value in the column is NULL, raise error\n",
    "            null_ratio = res_count_null/res_count\n",
    "            if null_ratio >= 0.5:\n",
    "                raise Exception(f\"{res_count_null} out of {res_count} records in {column} column of TABLE {table} is NULL!\\\n",
    "                \\n you may want to check the join opearation\")\n",
    "            \n",
    "    print(f\"congratulation! no error is found in join opearation in the TABLE {table}\")\n",
    "    \n",
    "    # print sample data\n",
    "    cur.execute(query_count_sample.format(table))\n",
    "    res_sample=cur.fetchmany(3)\n",
    "        \n",
    "    print(\"sample data:\")\n",
    "    for r_s in res_sample:\n",
    "            print(r_s)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congratulation! no error is found in uniqueness and completeness in the TABLE temperature_us_table\n",
      "sample data:\n",
      "('Maryland', 14.86, 0.26)\n",
      "('Oregon', 10.41, 0.31)\n",
      "('Connecticut', 12.04, 0.33)\n",
      "\n",
      "\n",
      "congratulation! no error is found in uniqueness and completeness in the TABLE demographics_us_table\n",
      "sample data:\n",
      "('Alabama', 36.16, 5163306, 352896, 252541, 2.43, 2448200, 2715106)\n",
      "('Minnesota', 35.58, 7044165, 321738, 1069888, 2.5, 3478803, 3565362)\n",
      "('Illinois', 35.71, 22514390, 723049, 4632600, 2.73, 10943864, 11570526)\n",
      "\n",
      "\n",
      "congratulation! no error is found in uniqueness and completeness in the TABLE traveler_table\n",
      "sample data:\n",
      "(459653, 'UNITED KINGDOM', 'ATLANTA, GA', datetime.date(2016, 4, 3), 'Air', 'FLORIDA', datetime.date(2016, 4, 13), 49, 'Pleasure', 1972, datetime.date(2016, 10, 2), 'M', None, 'B2')\n",
      "(459669, 'UNITED KINGDOM', 'ATLANTA, GA', datetime.date(2016, 4, 3), 'Air', 'GEORGIA', datetime.date(2016, 4, 6), 38, 'Business', 1983, datetime.date(2016, 7, 1), 'F', None, 'WB')\n",
      "(459685, 'UNITED KINGDOM', 'ATLANTA, GA', datetime.date(2016, 4, 3), 'Air', 'GEORGIA', datetime.date(2016, 4, 8), 63, 'Pleasure', 1958, datetime.date(2016, 7, 1), 'F', None, 'WT')\n",
      "\n",
      "\n",
      "congratulation! no error is found in uniqueness and completeness in the TABLE traveller_destination_table\n",
      "sample data:\n",
      "(459653, None, 'UNITED KINGDOM', 49, 1972, 'M', 'FLORIDA', 23.03, 32306132, 39.53, 7845566, 15461937, 16626425)\n",
      "(459669, None, 'UNITED KINGDOM', 38, 1983, 'F', 'GEORGIA', 16.28, 8555160, 33.81, 738925, 4101605, 4453555)\n",
      "(459685, None, 'UNITED KINGDOM', 63, 1958, 'F', 'GEORGIA', 16.28, 8555160, 33.81, 738925, 4101605, 4453555)\n",
      "\n",
      "\n",
      "congratulation! no error is found in join opearation in the TABLE traveller_destination_table\n",
      "sample data:\n",
      "(459653, None, 'UNITED KINGDOM', 49, 1972, 'M', 'FLORIDA', 23.03, 32306132, 39.53, 7845566, 15461937, 16626425)\n",
      "(459669, None, 'UNITED KINGDOM', 38, 1983, 'F', 'GEORGIA', 16.28, 8555160, 33.81, 738925, 4101605, 4453555)\n",
      "(459685, None, 'UNITED KINGDOM', 63, 1958, 'F', 'GEORGIA', 16.28, 8555160, 33.81, 738925, 4101605, 4453555)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dic_unique_count={\"temperature_us_table\" : \"state\",\n",
    "               \"demographics_us_table\" : \"state\",\n",
    "               \"traveler_table\" : \"cic_id\",\n",
    "               \"traveller_destination_table\" : \"cic_id\"}\n",
    "\n",
    "dic_join={\"traveller_destination_table\":[\"state_total_population\",\n",
    "                                            \"state_median_age\",\n",
    "                                            \"state_foreign_born\",\n",
    "                                            \"state_male_population\",\n",
    "                                            \"state_female_population\"]}\n",
    "\n",
    "check_unique_key(dic_unique_count,cur, conn)\n",
    "check_join(dic_join,cur, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "- Choice of technologies:\n",
    "\t- Spark for data lake: Spark is much quicker than Pandas when processing large datasets\n",
    "\t- Store the data as parquet at S3: saving as parquet saves storage space and quick to read\n",
    "\t- Data modelling with ETL pipeline using Redshift: well connected with S3 bucket and easy to operate using python\n",
    "- Update frequency: Once a month. Because the 2 data sources, \"I94 Immigration Data\" and \"World Temperature Data\", are both update or aggregate monthly, and \"U.S. City Demographic Data\" may not vary largely over month.\n",
    "\n",
    "Further development under different scenarios:\n",
    "- The data was increased by 100x: \n",
    "to improve performance, i will join all 3 data sources into one file in data lake by broadcast joining \"U.S. City Demographic Data\" and \"World Temperature Data\" and ripartition \"I94 Immigration Data\" by certain columns. \n",
    "At the same time, add number of clusters and improve their competence in Redshift, carefully set sort key and distribution key when creating tables to take advantages of the distribution features\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day: \n",
    "construct a pipeline using Airflow which permit operate the pipeline automatically every day at 7 am\n",
    "- The database needed to be accessed by 100+ people: \n",
    "create visitor role for Redshift cluster giving view and query access, then give the secret to authorized people so that they can access the database\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
